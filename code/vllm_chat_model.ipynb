{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import OpenAI\n",
    "from meeplemate.qa import build_sampling_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference_server_url = \"http://vllm:8001/v1\"\n",
    "\n",
    "# llm = OpenAI(\n",
    "#     model=\"teknium/OpenHermes-2.5-Mistral-7B\",\n",
    "#     openai_api_key=\"EMPTY\",\n",
    "#     openai_api_base=inference_server_url,\n",
    "#     # max_tokens=5,\n",
    "#     temperature=0,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/workspaces/mistral-rag-game-rules/code/.venv/lib/python3.10/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! logprobs is not default parameter.\n",
      "                logprobs was transferred to model_kwargs.\n",
      "                Please confirm that logprobs is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from meeplemate.llm_models import load_vllm_chat_model, load_tokenizer\n",
    "inference_server_url = \"http://vllm:8001/v1\"\n",
    "model_name = \"teknium/OpenHermes-2.5-Mistral-7B\"\n",
    "tokenizer = load_tokenizer(model_name)\n",
    "chat_model = load_vllm_chat_model(\n",
    "    inference_server_url=inference_server_url,\n",
    "    max_tokens=1000,\n",
    "    tokenizer=tokenizer,\n",
    "    logprobs=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[ChatGeneration(text=\"Hello! I'm doing well, thank you for asking. How can I assist you today?\", generation_info={'finish_reason': 'stop', 'logprobs': {'text_offset': [0, 5, 6, 8, 9, 10, 16, 21, 22, 28, 32, 36, 43, 44, 48, 52, 54, 61, 65, 71, 72], 'token_logprobs': [-0.7732282280921936, -0.002732830820605159, -0.0016049373662099242, -0.011095019057393074, -1.1920922133867862e-06, -0.288932204246521, -0.12895698845386505, -0.3503936231136322, -0.20799703896045685, -1.6689160474925302e-05, -0.07900013029575348, -0.00010513706365600228, -0.0002797450579237193, -0.0021432305220514536, -0.2846103608608246, -1.2993727978027891e-05, -0.5234487652778625, -3.313963316031732e-05, -0.0004182179400231689, -1.2278481335670222e-05, -0.01634553074836731], 'tokens': ['Hello', '!', '▁I', \"'\", 'm', '▁doing', '▁well', ',', '▁thank', '▁you', '▁for', '▁asking', '.', '▁How', '▁can', '▁I', '▁assist', '▁you', '▁today', '?', '<|im_end|>'], 'top_logprobs': [{'Hello': -0.7732282280921936}, {'!': -0.002732830820605159}, {'▁I': -0.0016049373662099242}, {\"'\": -0.011095019057393074}, {'m': -1.1920922133867862e-06}, {'▁doing': -0.288932204246521}, {'▁well': -0.12895698845386505}, {',': -0.3503936231136322}, {'▁thank': -0.20799703896045685}, {'▁you': -1.6689160474925302e-05}, {'▁for': -0.07900013029575348}, {'▁asking': -0.00010513706365600228}, {'.': -0.0002797450579237193}, {'▁How': -0.0021432305220514536}, {'▁can': -0.2846103608608246}, {'▁I': -1.2993727978027891e-05}, {'▁assist': -0.5234487652778625}, {'▁you': -3.313963316031732e-05}, {'▁today': -0.0004182179400231689}, {'?': -1.2278481335670222e-05}, {'<|im_end|>': -0.01634553074836731}]}}, message=AIMessage(content=\"Hello! I'm doing well, thank you for asking. How can I assist you today?\"))]], llm_output={}, run=[RunInfo(run_id=UUID('c3e5a851-2abd-401f-abe0-e8d5ffcbb024'))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage\n",
    "chat_model.generate([[HumanMessage(content=\"Hello, how are you?\")]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! I'm doing well, thank you for asking. How can I assist you today?\")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.invoke(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(AIMessage(content=\"Hello! I'm doing well, thank you for asking. How can I assist you today?\"),\n",
       " [-0.5928220748901367,\n",
       "  -5.245195097813848e-06,\n",
       "  -2.3841855067985307e-07,\n",
       "  -0.00020346954988781363,\n",
       "  0.0,\n",
       "  -0.21153756976127625,\n",
       "  -0.03849358111619949,\n",
       "  -0.16022755205631256,\n",
       "  -0.06236753985285759,\n",
       "  0.0,\n",
       "  -0.01416344940662384,\n",
       "  0.0,\n",
       "  -2.3841855067985307e-07,\n",
       "  -2.9802276912960224e-06,\n",
       "  -0.0788944810628891,\n",
       "  0.0,\n",
       "  -0.31326183676719666,\n",
       "  0.0,\n",
       "  -1.1920928244535389e-07,\n",
       "  0.0,\n",
       "  -0.00021944021864328533])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_sampling_chain(chat_model, temperature=0.5, logprobs=True).invoke(\"Hello, How are you!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=' Io amo il program')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful assistant that translates English to Italian.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"Translate the following sentence from English to Italian: I love programming.\"\n",
    "    ),\n",
    "]\n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To create a virtual environment in Python, you can use the `venv` module that comes with Python. Here are the steps to create a virtual environment:\n",
      "\n",
      "1. Open a terminal or command prompt.\n",
      "2. Navigate to the directory where you want to create the virtual environment.\n",
      "3. Run the following command to create the virtual environment:\n",
      "```\n",
      "python -m venv <env_name>\n",
      "```\n",
      "Replace `<env_name>` with the name you want to give to the virtual environment.\n",
      "\n",
      "4. Once the virtual environment is created, you can activate it by running the following command:\n",
      "```\n",
      "source <env_name>/bin/activate\n",
      "```\n",
      "On Windows, you can use the following command instead:\n",
      "```\n",
      "<env_name>\\Scripts\\activate\n",
      "```\n",
      "5. After activating the virtual environment, you can install packages and run your Python code as usual. When you're done working with the virtual environment, you can deactivate it by running the following command:\n",
      "```\n",
      "deactivate\n",
      "```\n",
      "On Windows, you can use the following command instead:\n",
      "```\n",
      "<env_name>\\Scripts\\deactivate\n",
      "```\n",
      "\n",
      "By using virtual environments, you can keep your Python projects organized and separate from each other, and avoid conflicts between different versions of packages.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful code assistant here to make my life easier.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"How do I create a python virutal environment?\"\n",
    "    ),\n",
    "]\n",
    "print(chat(messages).content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
