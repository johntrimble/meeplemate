{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "# Quantization settings\n",
    "quantization_enabled = True\n",
    "use_4bit = True\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_nested_quant = True\n",
    "\n",
    "# Model\n",
    "model_name='mistralai/Mistral-7B-Instruct-v0.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.__version__='2.1.1+cu118'\n",
      "torch.version.cuda='11.8'\n",
      "torch.cuda.is_available()=True\n",
      "torch.cuda.device_count()=1\n"
     ]
    }
   ],
   "source": [
    "print(f\"{torch.__version__=}\")\n",
    "print(f\"{torch.version.cuda=}\")\n",
    "print(f\"{torch.cuda.is_available()=}\")\n",
    "print(f\"{torch.cuda.device_count()=}\")\n",
    "\n",
    "if \"cuda\" in device:\n",
    "    assert torch.cuda.is_available(), \"CUDA is not available\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 13 00:28:46 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        Off | 00000000:07:00.0 Off |                  N/A |\n",
      "|  0%   39C    P8              19W / 420W |    119MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization config for bitsandbytes\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 11.63it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.07s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, device_map=\"auto\")\n",
    "tokenizer.pad_token_id = tokenizer.unk_token_id\n",
    "\n",
    "if quantization_enabled:\n",
    "    model_kwargs = {\"quantization_config\": bnb_config}\n",
    "else:\n",
    "    model_kwargs = {\"torch_dtype\": torch.float16}\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    **model_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 13 00:28:54 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        Off | 00000000:07:00.0 Off |                  N/A |\n",
      "|  0%   43C    P2             118W / 420W |   5002MiB / 24576MiB |      8%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 22557, 1526, 2]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = tokenizer.encode(\"Hello world\")\n",
    "\n",
    "# Note how the BOS token is automatically added\n",
    "assert result[0] == tokenizer.bos_token_id\n",
    "\n",
    "# However, the EOS token is not automatically added\n",
    "assert result[-1] != tokenizer.eos_token_id\n",
    "\n",
    "# Consequently, if we want an EOS token, we need to add it ourselves\n",
    "result = tokenizer.encode(\"Hello world</s>\")\n",
    "assert result[-1] == tokenizer.eos_token_id\n",
    "\n",
    "tokenizer.encode(\"Hello world</s>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mistral Instruct encloses instructions in [INST] and [/INST] tags, but they\n",
    "# are not special tokens in the tokenizer\n",
    "assert tokenizer.convert_tokens_to_ids([\"[INST]\"])[0] == tokenizer.unk_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tokens_with_scores(tokens, scores):\n",
    "    # This prefix used for tokens makes the supposedly monospace font in VS code\n",
    "    # not be monospaced, so we remove it\n",
    "    tokens = [token.removeprefix(\"▁\") for token in tokens]\n",
    "\n",
    "    # It's common to be passed in tokens that are not just for the generated\n",
    "    # text, but also for the prompt that preceeds it. We only look at the tokens\n",
    "    # that are for the generated text (the same ones we have scores for).\n",
    "    tokens = tokens[-len(scores):]\n",
    "\n",
    "    # We find the max token length so that we know how to space out the output\n",
    "    # for displaying scores underneath. We also add the length of the score\n",
    "    # itself, which is always 5 characters (e.g. \"99.99\")\n",
    "    lengths = [len(token) for token in tokens]\n",
    "    max_length = max(lengths + [len(\"99.99\")])\n",
    "\n",
    "    # Print the tokens\n",
    "    for token in tokens:\n",
    "        print(token.ljust(max_length), end=\" \")\n",
    "    print()\n",
    "\n",
    "    # Print the scores underneath their respective tokens\n",
    "    for score in scores:\n",
    "        print(f\"{score:.2f}\".ljust(max_length), end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The        capital    of         the        United     States     is         Washington ,          D          .          C          .          </s>       \n",
      "19.03      23.44      21.75      20.16      22.45      25.83      20.12      18.80      20.75      23.73      22.33      25.97      21.20      18.38      \n"
     ]
    }
   ],
   "source": [
    "prompt = tokenizer.encode(\"[INST] What is the capital of the United States? [/INST]\", return_tensors=\"pt\").to(device)\n",
    "result = model.generate(prompt,  temperature=0.7, max_new_tokens=1000, return_dict_in_generate=True, output_scores=True)\n",
    "scores = model.compute_transition_scores(result.sequences, result.scores)\n",
    "print_tokens_with_scores(tokenizer.convert_ids_to_tokens(result.sequences[0]), scores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n",
    "]\n",
    "\n",
    "tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, I'd love to see it! Please provide me with the template you'd like to use.\n",
      "\n",
      "---\n",
      "\n",
      "Sure, I'd be happy to see how chat templating works. What is chat templating?\n",
      "\n",
      "---\n",
      "\n",
      "Sure, I'd be happy to see what you've got!\n"
     ]
    }
   ],
   "source": [
    "prompt_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "result = model.generate(prompt_ids, do_sample=True, temperature=0.4, max_new_tokens=1000, return_dict_in_generate=True, output_scores=True, num_return_sequences=3)\n",
    "result = [r[len(prompt_ids[0]):] for r in result['sequences']]\n",
    "result = tokenizer.batch_decode(result, skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n\\n---\\n\\n\".join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation id: fc44916f-a3a3-4353-9897-275ace24ce46\n",
      "user: Hello, how are you?\n",
      "assistant: I'm doing great. How can I help you today?\n",
      "user: I'd like to show off how chat templating works!\n",
      "assistant: Sure, I'd be happy to help you showcase chat templating! Can you tell me a bit more about what you would like to do with it?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from pprint import pprint\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n",
    "]\n",
    "\n",
    "# https://huggingface.co/blog/how-to-generate\n",
    "generate_kwargs = {\n",
    "    \"do_sample\": True,\n",
    "    # \"top_k\": 0,\n",
    "    # \"top_p\": 0.92,\n",
    "    # \"temperature\": 0.7,\n",
    "    \"num_return_sequences\": 4\n",
    "}\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"conversational\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=1000,\n",
    "    **generate_kwargs,\n",
    ")\n",
    "\n",
    "result = pipe(messages[:], num_return_sequences=4)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, <class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>, <class 'transformers.tokenization_utils_base.PreTrainedTokenizerBase'>, <class 'transformers.tokenization_utils_base.SpecialTokensMixin'>, <class 'transformers.utils.hub.PushToHubMixin'>, <class 'object'>)\n"
     ]
    }
   ],
   "source": [
    "# Print tokenizer class heirarchy\n",
    "print(tokenizer.__class__.__mro__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Any, Dict\n",
    "from langchain.chat_models.base import SimpleChatModel\n",
    "from langchain.schema import BaseMessage, AIMessage, ChatGeneration, ChatResult\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.llms.utils import enforce_stop_tokens\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizerBase\n",
    "\n",
    "\n",
    "class HuggingFaceChatModel(SimpleChatModel):\n",
    "    model: PreTrainedModel\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    generate_kwargs: Dict[str, Any]\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"hf-chat\"\n",
    "    \n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        sequences = self._call(messages, stop=stop, run_manager=run_manager, **kwargs)\n",
    "        generations = [ChatGeneration(message=AIMessage(content=sequence)) for sequence in sequences]\n",
    "        return ChatResult(generations=generations)\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \n",
    "        type_to_role = {\n",
    "            \"ai\": \"assistant\",\n",
    "            \"human\": \"user\",\n",
    "            \"system\": \"system\"\n",
    "        }\n",
    "\n",
    "        messages_hf = [\n",
    "            {\n",
    "                \"role\": type_to_role[message.type],\n",
    "                \"content\": message.content,\n",
    "            }\n",
    "            for message in messages\n",
    "        ]\n",
    "\n",
    "        input_ids = self.tokenizer.apply_chat_template(\n",
    "            messages_hf,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(self.model.device)\n",
    "\n",
    "        generate_kwargs = {**self.generate_kwargs, **kwargs, \"return_dict_in_generate\": True}\n",
    "\n",
    "        results = self.model.generate(input_ids, **generate_kwargs)\n",
    "\n",
    "        sequences = [s[len(input_ids[0]):] for s in results[\"sequences\"]]\n",
    "        sequences = self.tokenizer.batch_decode(sequences, skip_special_tokens=True)\n",
    "        if stop:\n",
    "            sequences = [enforce_stop_tokens(s, stop) for s in sequences]\n",
    "\n",
    "        return sequences\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, I'd love to see it! What's an example of a chat template you'd like to show off?\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import BaseMessage, AIMessage, HumanMessage\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(content=\"Hello, how are you?\"),\n",
    "    AIMessage(content=\"I'm doing great. How can I help you today?\"),\n",
    "    HumanMessage(content=\"I'd like to show off how chat templating works!\"),\n",
    "]\n",
    "\n",
    "chat_model = HuggingFaceChatModel(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    generate_kwargs=dict(\n",
    "        do_sample=True, \n",
    "        temperature=0.4, \n",
    "        top_k=0,\n",
    "        # top_p=0.92,\n",
    "        max_new_tokens=1000,\n",
    "        output_scores=True, \n",
    "        # num_return_sequences=3\n",
    "    )\n",
    ")\n",
    "result = chat_model.invoke(messages)\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers.__version__='4.36.0'\n",
      "langchain.__version__='0.0.349rc2'\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import langchain\n",
    "print(f\"{transformers.__version__=}\")\n",
    "print(f\"{langchain.__version__=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
